{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context(\"poster\")\n",
    "sns.set(rc={\"figure.figsize\": (16, 9.)})\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", 120)\n",
    "pd.set_option(\"display.max_columns\", 120)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, stream=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.core import Root\n",
    "from snowflake import cortex\n",
    "from snowflake.snowpark import Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 3.8.1, Python Version: 3.10.9, Platform: macOS-14.4.1-arm64-arm-64bit\n",
      "INFO:snowflake.connector.connection:This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.\n",
      "INFO:snowflake.snowpark.session:Snowpark Session information: \n",
      "\"version\" : 1.15.0,\n",
      "\"python.version\" : 3.10.9,\n",
      "\"python.connector.version\" : 3.8.1,\n",
      "\"python.connector.session.id\" : 26798426968724674,\n",
      "\"os.name\" : Darwin\n",
      "\n",
      "INFO:snowflake.core.session._generated.api.session_api:Going to use client-REST for this resource\n"
     ]
    }
   ],
   "source": [
    "session = Session.builder.config('connection_name', 'default').create()\n",
    "root = Root(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../page.md', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "I am your student and I am learning how to use Snowflake with Python.\n",
    "You are a nice teacher that helps me, your student, to learn how to use Snowflake.\n",
    "\n",
    "You output a question, 4 answers, the correct answer, and an explanation in HTML format.\n",
    "Here is an example:\n",
    "\n",
    "<question>What is the purpose of the `Session.builder.configs` method in creating a session for Snowpark Python?</question>\n",
    "<answers>\n",
    "<ol type=\"1\">\n",
    "<li> To establish a connection to the Snowflake database </li>\n",
    "<li> To create a dictionary containing the names and values of the connection parameters </li>\n",
    "<li> To return a builder object that has the connection parameters </li>\n",
    "<li> To authenticate the user using the Snowflake Connector for Python  </li>\n",
    "</ol>\n",
    "</answers>\n",
    "<correct>To return a builder object that has the connection parameters</correct>\n",
    "<explanation>The `Session.builder.configs` method is used to create a builder object that has the connection parameters. This builder object can then be used to create a session object that can be used to execute queries against the Snowflake database. The method takes a dictionary of connection parameters as an argument, which contains the names and values of the parameters that are needed to connect to the database. The method returns a builder object that has the connection parameters, which can then be used to create a session object.</explanation>\n",
    "\n",
    "Take now the following page from the Snowflake documentation and formulate a question for me:\n",
    "{text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "\"Based on the following excerpt from the Snowflake documentation, generate a multiple-choice question that tests understanding of the key concept discussed. Include four answer choices and indicate the correct answer. Format the output in HTML.\n",
    "{text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"Based on the following excerpt from the Snowflake documentation, generate a multiple-choice question that tests understanding of the key concept discussed. Include four answer choices and indicate the correct answer. Format the output in HTML.\n",
      "[Developer](/en/developer)[Kafka and Spark Connectors](/en/user-guide/connectors)[Kafka Connector](/en/user-guide/kafka-connector)Overview\n",
      "\n",
      "Overview of the Kafka connector[¶](#overview-of-the-kafka-connector \"Link to this heading\")\n",
      "===========================================================================================\n",
      "\n",
      "\n",
      "This topic provides an overview of the Apache Kafka and the Snowflake Connector for Kafka.\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "\n",
      "\n",
      "The Kafka connector is subject to the [Connector Terms](https://www.snowflake.com/legal/snowflake-connector-terms).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Introduction to Apache Kafka[¶](#introduction-to-apache-kafka \"Link to this heading\")\n",
      "-------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Apache Kafka software uses a publish and subscribe model to write and read streams of records, similar to a message queue or enterprise messaging system. Kafka allows processes to read and write messages asynchronously. A subscriber does not need to be connected directly to a publisher; a publisher can queue a message in Kafka for the subscriber to receive later.\n",
      "\n",
      "\n",
      "An application publishes messages to a *topic*, and an application subscribes to a topic to receive those messages. Kafka can process, as well as transmit, messages; however, that is outside the scope of this document. Topics can be divided into *partitions* to increase scalability.\n",
      "\n",
      "\n",
      "Kafka Connect is a framework for connecting Kafka with external systems, including databases. A Kafka Connect cluster is a separate cluster from the Kafka cluster. The Kafka Connect cluster supports running and scaling out connectors (components that support reading and/or writing between external systems).\n",
      "\n",
      "\n",
      "The Kafka connector is designed to run in a Kafka Connect cluster to read data from Kafka topics and write the data into Snowflake tables.\n",
      "\n",
      "\n",
      "Snowflake provides two versions of the connector:\n",
      "\n",
      "\n",
      "* A version for the [Confluent package version of Kafka](https://www.confluent.io/hub/snowflakeinc/snowflake-kafka-connector/).\n",
      "\n",
      "\n",
      "For more information about Kafka Connect, see <https://docs.confluent.io/current/connect/>.\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "\n",
      "\n",
      "A hosted version of the Kafka connector is available in Confluent Cloud. For information, see <https://docs.confluent.io/current/cloud/connectors/cc-snowflake-sink.html>.\n",
      "* A version for the [open source software (OSS) Apache Kafka package](https://mvnrepository.com/artifact/com.snowflake/snowflake-kafka-connector/).\n",
      "\n",
      "\n",
      "For more information about Apache Kafka, see <https://kafka.apache.org/>.\n",
      "\n",
      "\n",
      "From the perspective of Snowflake, a Kafka topic produces a stream of rows to be inserted into a Snowflake table. In general, each Kafka message contains one row.\n",
      "\n",
      "\n",
      "Kafka, like many message publish/subscribe platforms, allows a many-to-many relationship between publishers and subscribers. A single application can publish to many\n",
      "topics, and a single application can subscribe to multiple topics. With Snowflake, the typical pattern is that one topic supplies messages (rows) for one Snowflake table.\n",
      "\n",
      "\n",
      "The current version of the Kafka connector is limited to loading data into Snowflake. The Kafka connector supports two data loading methods:\n",
      "\n",
      "\n",
      "* [Snowpipe](data-load-snowpipe-intro)\n",
      "* [Snowpipe Streaming](data-load-snowpipe-streaming-overview).\n",
      "\n",
      "\n",
      "For more information, refer to [Load Data into Snowflake](../guides-overview-loading-data) and [Using Snowflake Connector for Kafka With Snowpipe Streaming](data-load-snowpipe-streaming-kafka).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Target tables for Kafka topics[¶](#target-tables-for-kafka-topics \"Link to this heading\")\n",
      "-----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Kafka topics can be mapped to existing Snowflake tables in the Kafka configuration. If the topics are not mapped, then the Kafka connector creates a new table for each topic using the topic name.\n",
      "\n",
      "\n",
      "The connector converts the topic name to a valid Snowflake table name using the following rules:\n",
      "\n",
      "\n",
      "* Lowercase topic names are converted to uppercase table names.\n",
      "* If the first character in the topic name is not a letter (`a-z`, or `A-Z`) or an underscore character (`_`), then the connector prepends an underscore to the table name.\n",
      "* If any character inside the topic name is not a legal character for a Snowflake table name, then that character is replaced with the underscore character. For more information about which characters are valid in table names, see [Identifier requirements](../sql-reference/identifiers-syntax).\n",
      "\n",
      "\n",
      "Note that if the Kafka connector needs to adjust the name of the table created for a Kakfa topic, it is possible that the names of two tables in the same schema could be identical. For example, if you are reading data from topics `numbers+x` and `numbers-x`, the tables created for these topics would both be `NUMBERS_X`. To avoid accidental duplication of table names, the connector appends a suffix to the table name. The suffix is an underscore followed by a generated hash code.\n",
      "\n",
      "\n",
      "\n",
      "Tip\n",
      "\n",
      "\n",
      "Snowflake recommends that, when possible, you choose topic names that follow the rules for Snowflake identifier names.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Schema of tables for Kafka topics[¶](#schema-of-tables-for-kafka-topics \"Link to this heading\")\n",
      "-----------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "With [Snowpipe Streaming](data-load-snowpipe-streaming-overview), the Kafka connector optionally supports [schema detection and evolution](data-load-snowpipe-streaming-kafka-schema-detection).\n",
      "\n",
      "\n",
      "By default, with [Snowpipe](data-load-snowpipe-intro) or [Snowpipe Streaming](data-load-snowpipe-streaming-overview), every Snowflake table loaded by the Kafka connector has a schema consisting of two VARIANT columns:\n",
      "\n",
      "\n",
      "* RECORD\\_CONTENT. This contains the Kafka message.\n",
      "* RECORD\\_METADATA. This contains metadata about the message, for example, the topic from which the message was read.\n",
      "\n",
      "\n",
      "If Snowflake creates the table, then the table contains only these two columns. If the user creates the table for the\n",
      "Kafka Connector to add rows to, then the table can contain more than these two columns (any additional columns must\n",
      "allow NULL values because data from the connector does not include values for those columns).\n",
      "\n",
      "\n",
      "The RECORD\\_CONTENT column contains the Kafka message.\n",
      "\n",
      "\n",
      "A Kafka message has an internal structure that depends upon the information being sent. For example, a message from an IoT (Internet of Things) weather sensor\n",
      "might include the timestamp at which the data was recorded, the location of the sensor, the temperature, humidity, etc. A message from an inventory system\n",
      "might include the product ID and the number of items sold, perhaps along with a timestamp indicating when they were sold or shipped.\n",
      "\n",
      "\n",
      "Typically, each message in a specific topic has the same basic structure. Different topics typically use different structure.\n",
      "\n",
      "\n",
      "Each Kafka message is passed to Snowflake in JSON format or Avro format. The Kafka connector stores that formatted information in a single column of\n",
      "type [VARIANT](../sql-reference/data-types-semistructured.html#label-data-type-variant). The data is not parsed, and the data is not split into multiple columns in the Snowflake table.\n",
      "\n",
      "\n",
      "The RECORD\\_METADATA column contains the following information by default:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "| Field | Java . Data Type | SQL . Data Type | Required | Description |\n",
      "| --- | --- | --- | --- | --- |\n",
      "| topic | String | VARCHAR | Yes | The name of the Kafka topic that the record came from. |\n",
      "| partition | String | VARCHAR | Yes | The number of the partition within the topic. (Note that this is the Kafka partition, not the Snowflake micro-partition.) |\n",
      "| offset | long | INTEGER | Yes | The offset in that partition. |\n",
      "| CreateTime / . LogAppendTime | long | BIGINT | No | This is the timestamp associated with the message in the Kafka topic. The value is milliseconds since midnight January 1, 1970, UTC. For more information, see: <https://kafka.apache.org/0100/javadoc/org/apache/kafka/clients/producer/ProducerRecord.html> |\n",
      "| key | String | VARCHAR | No | If the message is a Kafka KeyedMessage, this is the key for that message. In order for the connector to store the key in the RECORD\\_METADATA, the key.converter parameter in the [Kafka configuration properties](kafka-connector-install.html#label-kafka-properties) must be set to “org.apache.kafka.connect.storage.StringConverter”; otherwise, the connector ignores keys. |\n",
      "| schema\\_id | int | INTEGER | No | When using Avro with a schema registry to specify a schema, this is the schema’s ID in that registry. |\n",
      "| headers | Object | OBJECT | No | A header is a user-defined key-value pair associated with the record. Each record can have 0, 1, or multiple headers. |\n",
      "\n",
      "\n",
      "The amount of metadata recorded in the RECORD\\_METADATA column is configurable using optional Kafka configuration properties. For information, see [Installing and configuring the Kafka connector](kafka-connector-install).\n",
      "\n",
      "\n",
      "The field names and values are case-sensitive.\n",
      "\n",
      "\n",
      "Expressed in JSON syntax, a sample message might look similar to the following:\n",
      "\n",
      "\n",
      "\n",
      "```\n",
      "{\n",
      "    \"meta\":\n",
      "    {\n",
      "        \"offset\": 1,\n",
      "        \"topic\": \"PressureOverloadWarning\",\n",
      "        \"partition\": 12,\n",
      "        \"key\": \"key name\",\n",
      "        \"schema_id\": 123,\n",
      "        \"CreateTime\": 1234567890,\n",
      "        \"headers\":\n",
      "        {\n",
      "            \"name1\": \"value1\",\n",
      "            \"name2\": \"value2\"\n",
      "        }\n",
      "    },\n",
      "    \"content\":\n",
      "    {\n",
      "        \"ID\": 62,\n",
      "        \"PSI\": 451,\n",
      "        \"etc\": \"...\"\n",
      "    }\n",
      "}\n",
      "\n",
      "```\n",
      "\n",
      "Copy\n",
      "You can query the Snowflake tables directly by using the appropriate [syntax for querying VARIANT columns](querying-semistructured).\n",
      "\n",
      "\n",
      "Here is a simple example of extracting data based on the topic in the RECORD\\_METADATA:\n",
      "\n",
      "\n",
      "\n",
      "```\n",
      "select\n",
      "       record_metadata:CreateTime,\n",
      "       record_content:ID\n",
      "    from table1\n",
      "    where record_metadata:topic = 'PressureOverloadWarning';\n",
      "\n",
      "```\n",
      "\n",
      "Copy\n",
      "The output would look similar to:\n",
      "\n",
      "\n",
      "\n",
      "```\n",
      "+------------+-----+\n",
      "| CREATETIME | ID  |\n",
      "+------------+-----+\n",
      "| 1234567890 | 62  |\n",
      "+------------+-----+\n",
      "\n",
      "```\n",
      "\n",
      "Copy\n",
      "Alternatively, you can extract the data from these tables, flatten the data into individual columns, and store the data in other tables, which typically are\n",
      "easier to query.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Workflow for the Kafka connector[¶](#workflow-for-the-kafka-connector \"Link to this heading\")\n",
      "---------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "The Kafka connector completes the following process to subscribe to Kafka topics and create Snowflake objects:\n",
      "\n",
      "\n",
      "1. The Kafka connector subscribes to one or more Kafka topics based on the configuration information provided via the Kafka configuration file or command line (or the Confluent Control Center; Confluent only).\n",
      "2. The connector creates the following objects for each topic:\n",
      "\n",
      "\n",
      "\t* One internal stage to temporarily store data files for each topic.\n",
      "\t* One pipe to ingest the data files for each topic partition.\n",
      "\t* One table for each topic. If the table specified for each topic does not exist, the connector creates it; otherwise, the connector creates the RECORD\\_CONTENT and RECORD\\_METADATA columns in the existing table and verifies that the other columns are nullable (and produces an error if they are not).\n",
      "\n",
      "\n",
      "The following diagram shows the ingest flow for Kafka with the Kafka connector:\n",
      "\n",
      "\n",
      "\n",
      "[![Kafka flow using the Kafka connector](../_images/kafka-connector-flow.png)](../_images/kafka-connector-flow.png)\n",
      "\n",
      "1. One or more applications publish JSON or Avro records to a Kafka cluster. The records are split into one or more topic partitions.\n",
      "2. The Kafka connector buffers messages from the Kafka topics. When a threshold (time or memory or number of messages) is reached, the connector writes the messages to a temporary file in the internal stage. The connector triggers [Snowpipe](data-load-snowpipe-intro) to ingest the temporary file. Snowpipe copies a pointer to the data file into a queue.\n",
      "3. A Snowflake-provided virtual warehouse loads data from the staged file into the target table (i.e. the table specified in the configuration file for the topic) via the pipe created for the Kafka topic partition.\n",
      "4. (Not shown) The connector monitors Snowpipe and deletes each file in the internal stage after confirming that the file data was loaded into the table.\n",
      "\n",
      "\n",
      "If a failure prevented the data from loading, the connector moves the file into the table stage and produces an error message.\n",
      "5. The connector repeats steps 2-4.\n",
      "\n",
      "\n",
      "\n",
      "Attention\n",
      "\n",
      "\n",
      "Snowflake polls the `insertReport` API for one hour. If the status of an ingested file does not\n",
      "succeed within this hour, the files being ingested are moved to a table stage.\n",
      "\n",
      "\n",
      "It may take at least one hour for these files to be available on the table stage. Files are\n",
      "only moved to the table stage when their ingestion status could not be found within the\n",
      "previous hour.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fault tolerance[¶](#fault-tolerance \"Link to this heading\")\n",
      "-----------------------------------------------------------\n",
      "\n",
      "\n",
      "Both Kafka and the Kafka connector are fault-tolerant. Messages are neither duplicated nor silently dropped.\n",
      "\n",
      "\n",
      "Data deduplication logic in the Snowpipe workflow in the data loading chain eliminates duplicate copies of repeating data except\n",
      "in rare cases. If an error is detected while Snowpipe loads a record (for example, the record was not well-formed JSON or Avro), then the\n",
      "record is not loaded; instead, the record is moved to a table stage.\n",
      "\n",
      "\n",
      "The Kafka connector with Snowpipe Streaming supports dead-letter queues (DLQ) for error handling. For more information, refer to [Error Handling and DLQ Properties for the Kafka Connector with Snowpipe Streaming](data-load-snowpipe-streaming-kafka.html#label-snowpipe-streaming-kafka-dlq-properties).\n",
      "\n",
      "\n",
      "\n",
      "### Limitations of fault tolerance with the connector[¶](#limitations-of-fault-tolerance-with-the-connector \"Link to this heading\")\n",
      "\n",
      "\n",
      "Kafka Topics can be configured with a limit on storage space or retention time.\n",
      "\n",
      "\n",
      "* The default retention time is 7 days. If the system is offline for more than the retention time, then expired records will\n",
      "not be loaded. Similarly, if Kafka’s storage space limit is exceeded, some messages will not be delivered.\n",
      "* If messages in the Kafka topic are deleted or updated, these changes might not be reflected in the Snowflake table.\n",
      "\n",
      "\n",
      "\n",
      "Attention\n",
      "\n",
      "\n",
      "Instances of the Kafka connector do not communicate with each other. If you start multiple instances of the connector on the\n",
      "same topics or partitions, then multiple copies of the same row might be inserted into the table. This is not recommended;\n",
      "each topic should be processed by only one instance of the connector.\n",
      "\n",
      "\n",
      "\n",
      "It is theoretically possible for messages to flow from Kafka faster than Snowflake can ingest them. In practice, however, this\n",
      "is unlikely. If it does occur, then solving the problem would require performance tuning of the Kafka Connect cluster. For\n",
      "example:\n",
      "\n",
      "\n",
      "* Tuning the number of nodes in the Connect cluster.\n",
      "* Tuning the number of tasks allocated to the connector.\n",
      "* Understanding the impact of the network bandwidth between the connector and the Snowflake deployment.\n",
      "\n",
      "\n",
      "\n",
      "Important\n",
      "\n",
      "\n",
      "There is no guarantee that rows are inserted in the order that they were originally published.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Supported platforms[¶](#supported-platforms \"Link to this heading\")\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "\n",
      "The Kafka connector can run in any Kafka Connect cluster, and can send data to a Snowflake account on any supported [cloud platform](intro-cloud-platforms).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Protobuf data support[¶](#protobuf-data-support \"Link to this heading\")\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Kafka connector 1.5.0 (or higher) supports protocol buffers (protobuf) via a protobuf converter. For details, see [Loading protobuf data using the Snowflake Connector for Kafka](kafka-connector-protobuf).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Billing information[¶](#billing-information \"Link to this heading\")\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "\n",
      "There is no direct charge for using the Kafka connector. However, there are indirect costs:\n",
      "\n",
      "\n",
      "* Snowpipe is used to load the data that the connector reads from Kafka, and Snowpipe processing time is charged to your account.\n",
      "* Data storage is charged to your account.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Kafka connector limitations[¶](#kafka-connector-limitations \"Link to this heading\")\n",
      "-----------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Single Message Transformations (SMTs) are applied to messages as they flow through Kafka Connect. When you configure the [Kafka configuration properties](kafka-connector-install.html#label-kafka-properties), if you set either `key.converter` or `value.converter` to one of the following values, then SMTs are not supported on the corresponding key or value:\n",
      "\n",
      "\n",
      "* `com.snowflake.kafka.connector.records.SnowflakeJsonConverter`\n",
      "* `com.snowflake.kafka.connector.records.SnowflakeAvroConverter`\n",
      "* `com.snowflake.kafka.connector.records.SnowflakeAvroConverterWithoutSchemaRegistry`\n",
      "\n",
      "\n",
      "When neither `key.converter` or `value.converter` is set, then most SMTs are supported, with the current exception of `regex.router`.\n",
      "\n",
      "\n",
      "Although the Snowflake converters do not support SMTs, Kafka connector version 1.4.3 (or higher) supports many community-based converters such as the following:\n",
      "\n",
      "\n",
      "* `io.confluent.connect.avro.AvroConverter`\n",
      "* `org.apache.kafka.connect.json.JsonConverter`\n",
      "\n",
      "\n",
      "For more information about SMTs, see <https://docs.confluent.io/current/connect/transforms/index.html>.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Was this page helpful?\n",
      "\n",
      "YesNo[Visit Snowflake](https://www.snowflake.com)[Join the conversation](https://community.snowflake.com/s/)[Develop with Snowflake](https://developers.snowflake.com)[Share your feedback](/feedback)[Read the latest on our blog](https://www.snowflake.com/blog/)[Get your own certification](https://learn.snowflake.com)[Privacy Notice](https://www.snowflake.com/privacy-policy/)[Site Terms](https://www.snowflake.com/legal/snowflake-site-terms/)© 2024 Snowflake, Inc. All Rights Reserved.On this page\n",
      "\n",
      "1. [Introduction to Apache Kafka](#introduction-to-apache-kafka)\n",
      "2. [Target tables for Kafka topics](#target-tables-for-kafka-topics)\n",
      "3. [Schema of tables for Kafka topics](#schema-of-tables-for-kafka-topics)\n",
      "4. [Workflow for the Kafka connector](#workflow-for-the-kafka-connector)\n",
      "5. [Fault tolerance](#fault-tolerance)\n",
      "6. [Supported platforms](#supported-platforms)\n",
      "7. [Protobuf data support](#protobuf-data-support)\n",
      "8. [Billing information](#billing-information)\n",
      "9. [Kafka connector limitations](#kafka-connector-limitations)\n",
      "Related content\n",
      "\n",
      "1. [Connector Terms](https://www.snowflake.com/legal/snowflake-connector-terms)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:snowflake.snowpark:Complete() is experimental since 1.0.12. Do not use it in production. \n",
      "INFO:snowflake.connector.cursor:Number of results in first chunk: 0\n"
     ]
    },
    {
     "ename": "SnowparkSQLException",
     "evalue": "(1300) (1304): 01b41ac4-0002-65f7-005f-35070001445e: 100351 (P0000): 01b41ac4-0002-65f7-005f-35070001445e: Request failed for external function COMPLETE$V2 with remote service error: 400 '\"max tokens of 4096 exceeded\"\n'; requests batch-id: 01b41ac4-0002-65f7-005f-35070001445e:1:1:0:0; request batch size: 1 rows; request retries: 0; response time (last retry): 44.53ms",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSnowparkSQLException\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Application Support/hatch/env/pip-compile/snow-instructor/CsuV6h9v/snow-instructor/lib/python3.10/site-packages/snowflake/ml/_internal/telemetry.py:367\u001b[0m, in \u001b[0;36msend_api_usage_telemetry.<locals>.decorator.<locals>.wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 367\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Library/Application Support/hatch/env/pip-compile/snow-instructor/CsuV6h9v/snow-instructor/lib/python3.10/site-packages/snowflake/cortex/_complete.py:26\u001b[0m, in \u001b[0;36mComplete\u001b[0;34m(model, prompt, session)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Complete calls into the LLM inference service to perform completion.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m    A column of string responses.\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_complete_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msnowflake.cortex.complete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Application Support/hatch/env/pip-compile/snow-instructor/CsuV6h9v/snow-instructor/lib/python3.10/site-packages/snowflake/cortex/_complete.py:35\u001b[0m, in \u001b[0;36m_complete_impl\u001b[0;34m(function, model, prompt, session)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_complete_impl\u001b[39m(\n\u001b[1;32m     30\u001b[0m     function: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     31\u001b[0m     model: Union[\u001b[38;5;28mstr\u001b[39m, snowpark\u001b[38;5;241m.\u001b[39mColumn],\n\u001b[1;32m     32\u001b[0m     prompt: Union[\u001b[38;5;28mstr\u001b[39m, snowpark\u001b[38;5;241m.\u001b[39mColumn],\n\u001b[1;32m     33\u001b[0m     session: Optional[snowpark\u001b[38;5;241m.\u001b[39mSession] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     34\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[\u001b[38;5;28mstr\u001b[39m, snowpark\u001b[38;5;241m.\u001b[39mColumn]:\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_sql_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Application Support/hatch/env/pip-compile/snow-instructor/CsuV6h9v/snow-instructor/lib/python3.10/site-packages/snowflake/cortex/_util.py:21\u001b[0m, in \u001b[0;36mcall_sql_function\u001b[0;34m(function, session, *args)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Union[\u001b[38;5;28mstr\u001b[39m, snowpark\u001b[38;5;241m.\u001b[39mColumn], call_sql_function_column(function, \u001b[38;5;241m*\u001b[39margs))\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(Union[\u001b[38;5;28mstr\u001b[39m, snowpark\u001b[38;5;241m.\u001b[39mColumn], \u001b[43mcall_sql_function_immediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Library/Application Support/hatch/env/pip-compile/snow-instructor/CsuV6h9v/snow-instructor/lib/python3.10/site-packages/snowflake/cortex/_util.py:42\u001b[0m, in \u001b[0;36mcall_sql_function_immediate\u001b[0;34m(function, session, *args)\u001b[0m\n\u001b[1;32m     41\u001b[0m df \u001b[38;5;241m=\u001b[39m empty_df\u001b[38;5;241m.\u001b[39mselect(functions\u001b[38;5;241m.\u001b[39mbuiltin(function)(\u001b[38;5;241m*\u001b[39mlit_args))\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(\u001b[38;5;28mstr\u001b[39m, \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/Library/Application Support/hatch/env/pip-compile/snow-instructor/CsuV6h9v/snow-instructor/lib/python3.10/site-packages/snowflake/snowpark/_internal/telemetry.py:144\u001b[0m, in \u001b[0;36mdf_collect_api_telemetry.<locals>.wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39mquery_history() \u001b[38;5;28;01mas\u001b[39;00m query_history:\n\u001b[0;32m--> 144\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m plan \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_select_statement \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_plan\n",
      "File \u001b[0;32m~/Library/Application Support/hatch/env/pip-compile/snow-instructor/CsuV6h9v/snow-instructor/lib/python3.10/site-packages/snowflake/snowpark/dataframe.py:597\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self, statement_params, block, log_on_exception, case_sensitive)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m open_telemetry_context_manager(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollect, \u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal_collect_with_tag_no_telemetry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstatement_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatement_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_on_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_on_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcase_sensitive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcase_sensitive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Application Support/hatch/env/pip-compile/snow-instructor/CsuV6h9v/snow-instructor/lib/python3.10/site-packages/snowflake/snowpark/dataframe.py:645\u001b[0m, in \u001b[0;36mDataFrame._internal_collect_with_tag_no_telemetry\u001b[0;34m(self, statement_params, block, data_type, log_on_exception, case_sensitive)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_internal_collect_with_tag_no_telemetry\u001b[39m(\n\u001b[1;32m    634\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;66;03m# we should always call this method instead of collect(), to make sure the\u001b[39;00m\n\u001b[1;32m    644\u001b[0m     \u001b[38;5;66;03m# query tag is set properly.\u001b[39;00m\n\u001b[0;32m--> 645\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_statement_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_or_update_statement_params_with_query_tag\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstatement_params\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_statement_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_tag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m            \u001b[49m\u001b[43mSKIP_LEVELS_THREE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_on_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_on_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcase_sensitive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcase_sensitive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Application Support/hatch/env/pip-compile/snow-instructor/CsuV6h9v/snow-instructor/lib/python3.10/site-packages/snowflake/snowpark/_internal/server_connection.py:510\u001b[0m, in \u001b[0;36mServerConnection.execute\u001b[0;34m(self, plan, to_pandas, to_iter, block, data_type, log_on_exception, case_sensitive, **kwargs)\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    508\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsync query is not supported in stored procedure yet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    509\u001b[0m     )\n\u001b[0;32m--> 510\u001b[0m result_set, result_meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result_set\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mto_pandas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43mto_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_on_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_on_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcase_sensitive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcase_sensitive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m block:\n",
      "File \u001b[0;32m~/Library/Application Support/hatch/env/pip-compile/snow-instructor/CsuV6h9v/snow-instructor/lib/python3.10/site-packages/snowflake/snowpark/_internal/analyzer/snowflake_plan.py:191\u001b[0m, in \u001b[0;36mSnowflakePlan.Decorator.wrap_exception.<locals>.wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m ne \u001b[38;5;241m=\u001b[39m SnowparkClientExceptionMessages\u001b[38;5;241m.\u001b[39mSQL_EXCEPTION_FROM_PROGRAMMING_ERROR(\n\u001b[1;32m    189\u001b[0m     e\n\u001b[1;32m    190\u001b[0m )\n\u001b[0;32m--> 191\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ne\u001b[38;5;241m.\u001b[39mwith_traceback(tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Application Support/hatch/env/pip-compile/snow-instructor/CsuV6h9v/snow-instructor/lib/python3.10/site-packages/snowflake/snowpark/_internal/analyzer/snowflake_plan.py:122\u001b[0m, in \u001b[0;36mSnowflakePlan.Decorator.wrap_exception.<locals>.wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m snowflake\u001b[38;5;241m.\u001b[39mconnector\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mProgrammingError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Library/Application Support/hatch/env/pip-compile/snow-instructor/CsuV6h9v/snow-instructor/lib/python3.10/site-packages/snowflake/snowpark/_internal/server_connection.py:612\u001b[0m, in \u001b[0;36mServerConnection.get_result_set\u001b[0;34m(self, plan, to_pandas, to_iter, block, data_type, log_on_exception, case_sensitive, **kwargs)\u001b[0m\n\u001b[1;32m    611\u001b[0m     final_query \u001b[38;5;241m=\u001b[39m final_query\u001b[38;5;241m.\u001b[39mreplace(holder, id_)\n\u001b[0;32m--> 612\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfinal_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mto_pandas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43mto_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mplan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqueries\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_ddl_on_temp_object\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_ddl_on_temp_object\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_last\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43masync_job_plan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_on_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_on_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcase_sensitive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcase_sensitive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    625\u001b[0m placeholders[query\u001b[38;5;241m.\u001b[39mquery_id_place_holder] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    626\u001b[0m     result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msfqid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last \u001b[38;5;28;01melse\u001b[39;00m result\u001b[38;5;241m.\u001b[39mquery_id\n\u001b[1;32m    627\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Application Support/hatch/env/pip-compile/snow-instructor/CsuV6h9v/snow-instructor/lib/python3.10/site-packages/snowflake/snowpark/_internal/server_connection.py:123\u001b[0m, in \u001b[0;36mServerConnection._Decorator.wrap_exception.<locals>.wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex\n",
      "File \u001b[0;32m~/Library/Application Support/hatch/env/pip-compile/snow-instructor/CsuV6h9v/snow-instructor/lib/python3.10/site-packages/snowflake/snowpark/_internal/server_connection.py:117\u001b[0m, in \u001b[0;36mServerConnection._Decorator.wrap_exception.<locals>.wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ReauthenticationRequest \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "File \u001b[0;32m~/Library/Application Support/hatch/env/pip-compile/snow-instructor/CsuV6h9v/snow-instructor/lib/python3.10/site-packages/snowflake/snowpark/_internal/server_connection.py:417\u001b[0m, in \u001b[0;36mServerConnection.run_query\u001b[0;34m(self, query, to_pandas, to_iter, is_ddl_on_temp_object, block, data_type, async_job_plan, log_on_exception, case_sensitive, params, num_statements, **kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to execute query\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery_id_log\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex\n\u001b[1;32m    419\u001b[0m \u001b[38;5;66;03m# fetch_pandas_all/batches() only works for SELECT statements\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;66;03m# We call fetchall() if fetch_pandas_all/batches() fails,\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;66;03m# because when the query plan has multiple queries, it will\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;66;03m# have non-select statements, and it shouldn't fail if the user\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;66;03m# calls to_pandas() to execute the query.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Application Support/hatch/env/pip-compile/snow-instructor/CsuV6h9v/snow-instructor/lib/python3.10/site-packages/snowflake/snowpark/_internal/server_connection.py:402\u001b[0m, in \u001b[0;36mServerConnection.run_query\u001b[0;34m(self, query, to_pandas, to_iter, is_ddl_on_temp_object, block, data_type, async_job_plan, log_on_exception, case_sensitive, params, num_statements, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[0;32m--> 402\u001b[0m     results_cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_and_notify_query_listener\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecute query [queryID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults_cursor\u001b[38;5;241m.\u001b[39msfqid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Application Support/hatch/env/pip-compile/snow-instructor/CsuV6h9v/snow-instructor/lib/python3.10/site-packages/snowflake/snowpark/_internal/server_connection.py:354\u001b[0m, in \u001b[0;36mServerConnection.execute_and_notify_query_listener\u001b[0;34m(self, query, **kwargs)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_and_notify_query_listener\u001b[39m(\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[1;32m    353\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SnowflakeCursor:\n\u001b[0;32m--> 354\u001b[0m     results_cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotify_query_listeners(\n\u001b[1;32m    356\u001b[0m         QueryRecord(results_cursor\u001b[38;5;241m.\u001b[39msfqid, results_cursor\u001b[38;5;241m.\u001b[39mquery)\n\u001b[1;32m    357\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Application Support/hatch/env/pip-compile/snow-instructor/CsuV6h9v/snow-instructor/lib/python3.10/site-packages/snowflake/connector/cursor.py:1080\u001b[0m, in \u001b[0;36mSnowflakeCursor.execute\u001b[0;34m(self, command, params, _bind_stage, timeout, _exec_async, _no_retry, _do_reset, _put_callback, _put_azure_callback, _put_callback_output_stream, _get_callback, _get_azure_callback, _get_callback_output_stream, _show_progress_bar, _statement_params, _is_internal, _describe_only, _no_results, _is_put_get, _raise_put_get_error, _force_put_overwrite, _skip_upload_on_content_match, file_stream, num_statements)\u001b[0m\n\u001b[1;32m   1079\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m IntegrityError \u001b[38;5;28;01mif\u001b[39;00m is_integrity_error \u001b[38;5;28;01melse\u001b[39;00m ProgrammingError\n\u001b[0;32m-> 1080\u001b[0m     \u001b[43mError\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrorhandler_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Library/Application Support/hatch/env/pip-compile/snow-instructor/CsuV6h9v/snow-instructor/lib/python3.10/site-packages/snowflake/connector/errors.py:290\u001b[0m, in \u001b[0;36mError.errorhandler_wrapper\u001b[0;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Error handler wrapper that calls the errorhandler method.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;124;03m    exception to the first handler in that order.\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 290\u001b[0m handed_over \u001b[38;5;241m=\u001b[39m \u001b[43mError\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhand_to_other_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m handed_over:\n",
      "File \u001b[0;32m~/Library/Application Support/hatch/env/pip-compile/snow-instructor/CsuV6h9v/snow-instructor/lib/python3.10/site-packages/snowflake/connector/errors.py:345\u001b[0m, in \u001b[0;36mError.hand_to_other_handler\u001b[0;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[1;32m    344\u001b[0m cursor\u001b[38;5;241m.\u001b[39mmessages\u001b[38;5;241m.\u001b[39mappend((error_class, error_value))\n\u001b[0;32m--> 345\u001b[0m \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrorhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Application Support/hatch/env/pip-compile/snow-instructor/CsuV6h9v/snow-instructor/lib/python3.10/site-packages/snowflake/connector/errors.py:221\u001b[0m, in \u001b[0;36mError.default_errorhandler\u001b[0;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[1;32m    220\u001b[0m done_format_msg \u001b[38;5;241m=\u001b[39m error_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone_format_msg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 221\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m error_class(\n\u001b[1;32m    222\u001b[0m     msg\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmsg\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    223\u001b[0m     errno\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m errno \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mint\u001b[39m(errno),\n\u001b[1;32m    224\u001b[0m     sqlstate\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqlstate\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    225\u001b[0m     sfqid\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msfqid\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    226\u001b[0m     query\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    227\u001b[0m     done_format_msg\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    228\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m done_format_msg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(done_format_msg)\n\u001b[1;32m    229\u001b[0m     ),\n\u001b[1;32m    230\u001b[0m     connection\u001b[38;5;241m=\u001b[39mconnection,\n\u001b[1;32m    231\u001b[0m     cursor\u001b[38;5;241m=\u001b[39mcursor,\n\u001b[1;32m    232\u001b[0m )\n",
      "\u001b[0;31mSnowparkSQLException\u001b[0m: (1304): 01b41ac4-0002-65f7-005f-35070001445e: 100351 (P0000): 01b41ac4-0002-65f7-005f-35070001445e: Request failed for external function COMPLETE$V2 with remote service error: 400 '\"max tokens of 4096 exceeded\"\n'; requests batch-id: 01b41ac4-0002-65f7-005f-35070001445e:1:1:0:0; request batch size: 1 rows; request retries: 0; response time (last retry): 44.53ms",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mSnowparkSQLException\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msnowflake-arctic\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# model = 'llama2-70b-chat'\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mcortex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mComplete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Application Support/hatch/env/pip-compile/snow-instructor/CsuV6h9v/snow-instructor/lib/python3.10/site-packages/snowflake/snowpark/_internal/utils.py:676\u001b[0m, in \u001b[0;36mfunc_decorator.<locals>.wrapper.<locals>.func_call_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc_call_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    675\u001b[0m     warning(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m, warning_text)\n\u001b[0;32m--> 676\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Application Support/hatch/env/pip-compile/snow-instructor/CsuV6h9v/snow-instructor/lib/python3.10/site-packages/snowflake/ml/_internal/telemetry.py:389\u001b[0m, in \u001b[0;36msend_api_usage_telemetry.<locals>.decorator.<locals>.wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m me\u001b[38;5;241m.\u001b[39moriginal_exception \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 389\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m me\u001b[38;5;241m.\u001b[39moriginal_exception \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m update_stmt_params_if_snowpark_df(res, statement_params)\n",
      "\u001b[0;31mSnowparkSQLException\u001b[0m: (1300) (1304): 01b41ac4-0002-65f7-005f-35070001445e: 100351 (P0000): 01b41ac4-0002-65f7-005f-35070001445e: Request failed for external function COMPLETE$V2 with remote service error: 400 '\"max tokens of 4096 exceeded\"\n'; requests batch-id: 01b41ac4-0002-65f7-005f-35070001445e:1:1:0:0; request batch size: 1 rows; request retries: 0; response time (last retry): 44.53ms"
     ]
    }
   ],
   "source": [
    "model = 'snowflake-arctic'\n",
    "# model = 'llama2-70b-chat'\n",
    "answer = cortex.Complete(model=model, prompt=prompt, session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'answer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43manswer\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'answer' is not defined"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Which of the following is the correct way to create a session for Snowpark Python?\n",
      "\n",
      "A. `from snowflake.snowpark import Session`\n",
      "B. `Session.builder.configs(connection_parameters).create()`\n",
      "C. `new_session = Session.builder.configs(connection_parameters).create()`\n",
      "D. `new_session = Session.builder.create(connection_parameters)`\n",
      "\n",
      "Correct answer: C. `new_session = Session.builder.configs(connection_parameters).create()`\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
